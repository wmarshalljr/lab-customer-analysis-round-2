{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUND 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions\n",
    "\n",
    "# Show the dataframe shape.\n",
    "# Standardize header names.\n",
    "# Which columns are numerical?\n",
    "# Which columns are categorical?\n",
    "# Check and deal with NaN values.\n",
    "# Datetime format - Extract the months from the dataset and store in a separate column. \n",
    "# Then filter the data to show only the information for the first quarter , ie. January, February and March. \n",
    "# Hint: If data from March does not exist, consider only January and February."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('marketing_customer_analysis.csv')\n",
    "\n",
    "data = data.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "data # This also shows the shape (10910 x 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns) # Look for standardized headers. The dataset headers seem okay already? Perhaps lowercase?\n",
    "\n",
    "cols = []\n",
    "for i in range(len(data.columns)):\n",
    "    cols.append(data.columns[i].lower().replace(' ','_')) # I've added the snake, using .replace('', '_')\n",
    "data.columns = cols\n",
    "\n",
    "data # To confirm the for loop worked on the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dtypes)\n",
    "\n",
    "# Categorical: All columns with 'object' type\n",
    "\n",
    "# Numerical: All columns with 'float64' and 'int64' types\n",
    "\n",
    "# Here we can also pull out numerical and categorical columns and filter / print them \n",
    "\n",
    "# for example: isNumeric = is_numeric(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data.duplicated()) # check for duplicates\n",
    "\n",
    "data.drop_duplicates(inplace=True) # remove duplicates\n",
    "\n",
    "sum(data.duplicated()) # confirm that duplicates have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at data for how to address null values\n",
    "\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(data.isna().sum()/len(data),4)*100  # shows the percentage of null values in a column\n",
    "nulls_df = pd.DataFrame(round(data.isna().sum()/len(data),4)*100)\n",
    "nulls_df\n",
    "nulls_df = nulls_df.reset_index()\n",
    "nulls_df\n",
    "nulls_df.columns = ['header_name', 'percent_nulls']\n",
    "nulls_df\n",
    "\n",
    "# Based on the outputs of null_df, I would prefer to remove the 'vehicle class' column altogether and drop rows with >0 nulls \n",
    "# (It's a large dataset so should not introduce any bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['state'].value_counts() # Judging by this quick look at the data, probably best to just remove rows with NaN than to replace with data (especially since it is categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data = data.drop('vehicle_type', axis=1) # First remove this column so the next command does not drop about 5,000 rows from the data set\n",
    "\n",
    "data_clean = data.dropna(axis=0) # This is the simplest and most effective to remove all rows with NaN \n",
    "\n",
    "data_clean.isna().sum() # Confirm NaNs are removed from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.reset_index(drop=True) # Make sure the new 'clean' dataset looks okay before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['effective_to_date'] = pd.to_datetime(data_clean['effective_to_date'], errors='coerce')\n",
    "\n",
    "data_clean['month'] = pd.DatetimeIndex(data_clean['effective_to_date']).month\n",
    "\n",
    "data_clean\n",
    "\n",
    "#data_clean.sort_values('effective to date') # Based on the df, it already only includes data from January and February??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract months from dataset and create new column\n",
    "\n",
    "data_clean_Q1 = data_clean[(data_clean['month']<4)]\n",
    "\n",
    "data_clean_Q1 # Confirming that only Q1 data is in the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS - Function for all cleaning / pre-processing\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUND 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions \n",
    "\n",
    "# EDA (Exploratory Data Analysis) - Complete the following tasks to explore the data:\n",
    "# Show DataFrame info.\n",
    "# Describe DataFrame.\n",
    "# Show a plot of the total number of responses.\n",
    "# Show a plot of the response rate by the sales channel.\n",
    "# Show a plot of the response rate by the total claim amount.\n",
    "# Show a plot of the response rate by income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.describe().T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_count = data_clean['response'].value_counts()\n",
    "\n",
    "response_count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First option for sales channel\n",
    "\n",
    "# data_clean.groupby('sales channel')['response_count'].mean().plot.bar()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# Other option for sales channel... sns.countplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['response_binary'] = data_clean['response'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "data_clean.reset_index(drop=True) \n",
    "\n",
    "data_clean.head(10) # Test to make sure the map function passed the dictionary to the new list response_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_channel_response_rate = data_clean.groupby('response_binary')['sales_channel'].value_counts(normalize=True).unstack('sales channel')\n",
    "\n",
    "sales_channel_response_rate\n",
    "\n",
    "sales_channel_response_rate.plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='response_binary', y='total_claim_amount', data=data_clean)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='response_binary', y='income', data=data_clean)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUND 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = data_clean.select_dtypes(include= np.number)\n",
    "\n",
    "data_num = data_num.drop(['response_binary'], axis=1)\n",
    "\n",
    "data_cat = data_clean.select_dtypes(include= object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data_num['income']) # Example of one column distribution plot...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all of the columns in data_num all at once\n",
    "\n",
    "for i, column in enumerate(data_num.columns, 1):\n",
    "    sns.displot(data_num[column])\n",
    "\n",
    "# Not all of the variable plots are normally distributed. \n",
    "# For instance, month is bimodal (by design)\n",
    "# months_since_policy_inception may be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using Matplotlib - not a best practice to pass same number of bins for all variables \n",
    "# But this is the simplest way I could find to do this\n",
    "\n",
    "data_num.hist(bins=25, figsize=(30, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for multicollinearity \n",
    "\n",
    "correlations_matrix = data_num.corr()\n",
    "sns.set(rc = {'figure.figsize':(10,5)})\n",
    "sns.heatmap(correlations_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check why total claim amount and income have a moderate negative correlation \n",
    "# (just out of curiosity).\n",
    "\n",
    "sns.scatterplot(x='total_claim_amount', y='income', data=data_num)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we be removing the outliers at this stage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUND 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_clean['total_claim_amount']\n",
    "X = data_num.drop(['total_claim_amount'], axis=1)\n",
    "# X_num = X.select_dtypes(include = np.number). Saving these for later...\n",
    "# X_cat = X.select_dtypes(include = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "lm.fit(X,y)\n",
    "print(lm.score(X,y))\n",
    "y_pred = lm.predict(X)\n",
    "print(mean_squared_error(y_pred,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.coef_) \n",
    "\n",
    "# Checking coefficients. Need to know p values for each to test significance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = MinMaxScaler().fit(X)\n",
    "x_normalized = transformer.transform(X)\n",
    "print(x_normalized.shape)\n",
    "x_normalized\n",
    "\n",
    "# Use MinMaxScaler to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num_normalized = pd.DataFrame(x_normalized, columns=X.columns) # pass this into a df so we can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num_normalized.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=X['income'], y=y)\n",
    "\n",
    "# Just visualising one of the relationships to see what it looks like."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
